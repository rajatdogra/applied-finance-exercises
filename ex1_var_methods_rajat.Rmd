---
title: "Risk Assessment via VaR Methodologies"
author: "Rajat Dogra 474072"
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  message = FALSE,
  fig.align = 'center',
  fig.width = 10,
  fig.height = 5
)
```

# Introduction

This exercise evaluates three distinct Value-at-Risk (VaR) methodologies using historical data from the Warsaw Stock Exchange WIG index. The objective is to assess which approach provides superior risk estimates during the 2021 calendar year.

**Research Questions:**

- How do different VaR models perform when tested on out-of-sample data?
- Which methodology adapts best to changing market volatility?
- What underlying factors drive performance differences?

# Methodology Overview

Three competing approaches are examined:

| Method | Type | Key Feature |
|--------|------|-------------|
| Historical Simulation | Non-parametric | Uses raw historical quantiles |
| GARCH Modeling | Parametric | Models time-varying volatility |
| Filtered HS | Semi-parametric | Combines GARCH with empirical tails |

All models employ a **rolling window** approach with 500-day estimation periods.

# Data Import and Processing

```{r load_libraries, message=FALSE}
suppressPackageStartupMessages({
  library(quantmod)
  library(rugarch)
  library(xts)
  library(PerformanceAnalytics)
  library(dplyr)
  library(ggplot2)
})
```

## Loading WIG Index Data

```{r import}
# Read WIG index data
wig_data <- read.csv("wig.csv")

# Convert date column
wig_data$Data <- as.Date(wig_data$Data)

# Create time series object
wig_prices <- xts(wig_data$Zamkniecie, order.by = wig_data$Data)

# Focus on recent history with sufficient data
wig_prices <- wig_prices["2000/"]

cat("Data range:", as.character(start(wig_prices)), "to", as.character(end(wig_prices)))
cat("\nTotal observations:", nrow(wig_prices))
```

## Return Calculation

```{r compute_returns}
# Compute log returns
log_returns <- diff(log(wig_prices))
log_returns <- na.omit(log_returns)

# Display basic statistics
cat("\nReturn Statistics:\n")
cat("Mean:", round(mean(log_returns) * 100, 4), "%\n")
cat("Volatility:", round(sd(log_returns) * 100, 2), "%\n")
cat("Min:", round(min(log_returns) * 100, 2), "%\n")
cat("Max:", round(max(log_returns) * 100, 2), "%\n")
```

## Train-Test Split

```{r split}
# Training: Everything before 2021
train_set <- log_returns["/2020-12-31"]

# Testing: Year 2021
test_set <- log_returns["2021-01-01/2021-12-31"]

cat("Training observations:", length(train_set))
cat("\nTesting observations:", length(test_set))
```

## Preliminary Visualization

```{r visual_exploration, fig.height=7}
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# Panel 1: Price evolution
plot(wig_prices["2015/"], main = "WIG Index (2015-2021)", 
     col = "navy", lwd = 1.2, ylab = "Index Level")
grid()

# Panel 2: Return series
plot(log_returns["2015/"], main = "Daily Log Returns", 
     col = "darkgreen", lwd = 0.7, ylab = "Return")
abline(h = 0, col = "red", lty = 2)
grid()

# Panel 3: Histogram
hist(log_returns, breaks = 60, col = "skyblue", border = "white",
     main = "Return Distribution", xlab = "Log Return", prob = TRUE)
curve(dnorm(x, mean(log_returns), sd(log_returns)), 
      add = TRUE, col = "red", lwd = 2)

# Panel 4: Volatility clustering check
acf(log_returns^2, main = "ACF of Squared Returns", lag.max = 40)

par(mfrow = c(1, 1))
```

**Observations:**

- Returns display pronounced **volatility clustering**
- Distribution exhibits **heavy tails** relative to normal
- Autocorrelation in squared returns confirms **ARCH effects**

These stylized facts motivate the use of GARCH-family models.

# Model Specifications

```{r model_params}
# Configuration
WINDOW_LENGTH <- 500    # Rolling window size
CONFIDENCE <- 0.99      # 99% confidence level
ALPHA <- 1 - CONFIDENCE # Tail probability
```

# Implementation

## Approach 1: Historical Simulation

This baseline method computes VaR as the empirical $\alpha$-quantile from the rolling historical window.

**Mathematical formulation:**
$$\text{VaR}_t^{HS} = Q_{\alpha}(r_{t-500:t-1})$$

```{r method1}
# Compute rolling empirical quantile
var_historical <- rollapply(
  log_returns,
  width = WINDOW_LENGTH,
  FUN = function(x) quantile(x, probs = ALPHA, na.rm = TRUE),
  align = "right",
  fill = NA
)

# Extract 2021 forecasts
var_hist_test <- var_historical["2021"]
```

**Characteristics:**

- Distribution-free approach
- Captures historical extreme events
- Slow adaptation to regime changes
- Equal weighting of all observations

## Approach 2: GARCH(1,1) Model

This parametric approach models conditional heteroskedasticity:

**Model equations:**
$$r_t = \mu + \varepsilon_t$$
$$\varepsilon_t = \sigma_t \cdot z_t, \quad z_t \stackrel{iid}{\sim} N(0,1)$$
$$\sigma_t^2 = \omega + \alpha_1 \varepsilon_{t-1}^2 + \beta_1 \sigma_{t-1}^2$$

**VaR calculation:**
$$\text{VaR}_t^{GARCH} = \hat{\mu}_t + \hat{\sigma}_t \cdot \Phi^{-1}(\alpha)$$

```{r method2}
# Define model specification
model_spec <- ugarchspec(
  mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),
  variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
  distribution.model = "norm"
)

# Initialize results container
n_forecasts <- length(test_set)
var_garch_values <- numeric(n_forecasts)

# Rolling estimation and forecasting
for (idx in 1:n_forecasts) {
  
  # Define estimation window
  forecast_date <- index(test_set)[idx]
  historical_data <- log_returns[index(log_returns) < forecast_date]
  estimation_window <- tail(historical_data, WINDOW_LENGTH)
  
  # Estimate model
  garch_fit <- tryCatch({
    ugarchfit(spec = model_spec, data = estimation_window, 
              solver = "hybrid", silent = TRUE)
  }, error = function(e) NULL)
  
  # Generate forecast if successful
  if (!is.null(garch_fit)) {
    one_step <- ugarchforecast(garch_fit, n.ahead = 1)
    mu_forecast <- fitted(one_step)[1]
    sigma_forecast <- sigma(one_step)[1]
    var_garch_values[idx] <- mu_forecast + sigma_forecast * qnorm(ALPHA)
  } else {
    var_garch_values[idx] <- NA
  }
}

# Convert to xts
var_garch_test <- xts(var_garch_values, order.by = index(test_set))
```

**Characteristics:**

- Captures volatility persistence
- Fast adaptation to volatility shifts
- Assumes normal innovations (may miss tail risk)
- Requires parameter estimation

## Approach 3: Filtered Historical Simulation

Hybrid methodology combining GARCH volatility dynamics with non-parametric tail estimation.

**Procedure:**

1. Fit GARCH model and extract standardized residuals: $\hat{z}_t = \hat{\varepsilon}_t / \hat{\sigma}_t$
2. Compute empirical quantile: $q_{\alpha} = Q_{\alpha}(\{\hat{z}_t\})$
3. Scale by forecasted volatility: $\text{VaR}_t^{FHS} = \hat{\sigma}_{t|t-1} \times q_{\alpha}$

```{r method3}
# Initialize results
var_fhs_values <- numeric(n_forecasts)

# Rolling procedure
for (idx in 1:n_forecasts) {
  
  forecast_date <- index(test_set)[idx]
  historical_data <- log_returns[index(log_returns) < forecast_date]
  estimation_window <- tail(historical_data, WINDOW_LENGTH)
  
  # Fit GARCH
  garch_fit <- tryCatch({
    ugarchfit(spec = model_spec, data = estimation_window,
              solver = "hybrid", silent = TRUE)
  }, error = function(e) NULL)
  
  if (!is.null(garch_fit)) {
    # Extract standardized residuals
    standardized_resid <- residuals(garch_fit, standardize = TRUE)
    
    # Empirical tail quantile
    tail_quantile <- quantile(standardized_resid, probs = ALPHA, na.rm = TRUE)
    
    # Volatility forecast (use last fitted value as proxy)
    vol_forecast <- as.numeric(tail(sigma(garch_fit), 1))
    
    # Compute VaR
    var_fhs_values[idx] <- vol_forecast * tail_quantile
  } else {
    var_fhs_values[idx] <- NA
  }
}

var_fhs_test <- xts(var_fhs_values, order.by = index(test_set))
```

**Characteristics:**

- Adaptive volatility from GARCH
- Realistic tail behavior from empirical distribution
- Best of both parametric and non-parametric worlds
- Computationally intensive

# Results and Comparison

## Visual Comparison

```{r main_plot, fig.width=12, fig.height=6}
# Combine all series
comparison_df <- data.frame(
  Date = index(test_set),
  Actual = as.numeric(test_set),
  HS = as.numeric(var_hist_test),
  GARCH = as.numeric(var_garch_test),
  FHS = as.numeric(var_fhs_test)
)

# Create plot
ggplot(comparison_df, aes(x = Date)) +
  geom_line(aes(y = Actual, color = "Actual Returns"), linewidth = 0.8) +
  geom_line(aes(y = HS, color = "Historical Sim"), linewidth = 1.1) +
  geom_line(aes(y = GARCH, color = "GARCH"), linewidth = 1.1) +
  geom_line(aes(y = FHS, color = "Filtered HS"), linewidth = 1.1) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  scale_color_manual(values = c(
    "Actual Returns" = "black",
    "Historical Sim" = "tomato",
    "GARCH" = "steelblue",
    "Filtered HS" = "forestgreen"
  )) +
  labs(
    title = "VaR Model Comparison: WIG Index (2021)",
    subtitle = "99% Confidence Level | 500-Day Rolling Window",
    x = NULL,
    y = "Log Return / VaR Estimate",
    color = "Series"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 14),
    panel.grid.minor = element_blank()
  )
```

**Visual interpretation:**

- Historical Simulation produces relatively **static** estimates
- GARCH and FHS show **dynamic** adaptation to volatility
- FHS tends to be slightly more **conservative** than GARCH

## Backtesting Analysis

### Violation Count

```{r violation_count}
# Identify exceedances
exceed_hs <- sum(test_set < var_hist_test, na.rm = TRUE)
exceed_garch <- sum(test_set < var_garch_test, na.rm = TRUE)
exceed_fhs <- sum(test_set < var_fhs_test, na.rm = TRUE)

# Expected number
n_valid <- length(test_set)
expected_exceed <- n_valid * ALPHA

# Compile results
backtest_summary <- data.frame(
  Method = c("Historical Simulation", "GARCH(1,1)", "Filtered HS"),
  Exceedances = c(exceed_hs, exceed_garch, exceed_fhs),
  Expected = rep(round(expected_exceed, 1), 3),
  `Empirical Rate` = paste0(
    round(c(exceed_hs, exceed_garch, exceed_fhs) / n_valid * 100, 2), "%"
  ),
  check.names = FALSE
)

knitr::kable(backtest_summary, align = 'c',
             caption = "Violation Statistics (2021)")
```

### Statistical Testing: Kupiec LR Test

The Kupiec test evaluates whether observed violation rates are statistically consistent with the target level.

**Hypotheses:**

- $H_0$: Violation rate = $\alpha$ (model calibrated correctly)
- $H_1$: Violation rate $\neq \alpha$

**Test statistic:**
$$LR = -2 \left[ \ell(\alpha) - \ell(\hat{p}) \right] \sim \chi^2_1$$

```{r kupiec_test}
# Function to perform Kupiec test
perform_kupiec <- function(alpha_level, n_violations, sample_size) {
  
  # Empirical violation rate
  p_empirical <- n_violations / sample_size
  
  # Handle boundary cases
  if (n_violations == 0 || n_violations == sample_size) {
    return(data.frame(
      LR_Stat = NA,
      P_Value = NA,
      Decision = "Boundary"
    ))
  }
  
  # Log-likelihood ratio statistic
  log_lik_h0 <- (sample_size - n_violations) * log(1 - alpha_level) + 
                n_violations * log(alpha_level)
  
  log_lik_h1 <- (sample_size - n_violations) * log(1 - p_empirical) + 
                n_violations * log(p_empirical)
  
  lr_statistic <- -2 * (log_lik_h0 - log_lik_h1)
  
  # P-value
  p_val <- 1 - pchisq(lr_statistic, df = 1)
  
  # Decision at 5% significance
  decision <- ifelse(p_val > 0.05, "Accept H0", "Reject H0")
  
  return(data.frame(
    LR_Stat = round(lr_statistic, 3),
    P_Value = round(p_val, 4),
    Decision = decision
  ))
}

# Apply to each model
kupiec_hs <- perform_kupiec(ALPHA, exceed_hs, n_valid)
kupiec_garch <- perform_kupiec(ALPHA, exceed_garch, n_valid)
kupiec_fhs <- perform_kupiec(ALPHA, exceed_fhs, n_valid)

# Combine results
statistical_tests <- data.frame(
  Method = c("Historical Simulation", "GARCH(1,1)", "Filtered HS"),
  LR_Statistic = c(kupiec_hs$LR_Stat, kupiec_garch$LR_Stat, kupiec_fhs$LR_Stat),
  `P-Value` = c(kupiec_hs$P_Value, kupiec_garch$P_Value, kupiec_fhs$P_Value),
  `Test Result` = c(kupiec_hs$Decision, kupiec_garch$Decision, kupiec_fhs$Decision),
  check.names = FALSE
)

knitr::kable(statistical_tests, align = 'c',
             caption = "Kupiec Unconditional Coverage Test Results")
```

**Interpretation guide:**

- P-value > 0.05 → Model passes (violation rate consistent with target)
- P-value ≤ 0.05 → Model fails (systematic bias in risk estimates)

## Model Dynamics

```{r dynamics}
# Calculate VaR variability
var_volatility <- data.frame(
  Method = c("Historical Simulation", "GARCH(1,1)", "Filtered HS"),
  `Mean VaR` = c(
    mean(var_hist_test, na.rm = TRUE),
    mean(var_garch_test, na.rm = TRUE),
    mean(var_fhs_test, na.rm = TRUE)
  ),
  `Std Dev` = c(
    sd(var_hist_test, na.rm = TRUE),
    sd(var_garch_test, na.rm = TRUE),
    sd(var_fhs_test, na.rm = TRUE)
  ),
  check.names = FALSE
)

# Convert to percentage points
var_volatility[, 2:3] <- var_volatility[, 2:3] * 100

knitr::kable(var_volatility, digits = 3, align = 'c',
             caption = "VaR Estimate Dynamics (%)")
```

Lower standard deviation in HS reflects its **inertia** in responding to volatility changes.

# Discussion

## Q1: Does model quality change over time?
  
**Answer: Yes, substantially.**

Model performance is **time-varying** and depends on market conditions:

- **Stable periods:** All methods converge to similar estimates
- **Volatile periods:** Models diverge significantly
  - HS remains relatively flat (lag effect)
  - GARCH/FHS adjust rapidly upward

**2021 Context:**

- Post-pandemic recovery phase
- Central bank policy uncertainty
- Elevated and fluctuating volatility
- Perfect environment to test model adaptability

## Q2: Which approach performs best?

**Ranking (best to worst):**

**1. Filtered Historical Simulation**

- Optimal balance: adaptive volatility + realistic tails
- Generally passes Kupiec test
- Most responsive during stress while avoiding false alarms

**2. GARCH(1,1)**

- Strong improvement over naive HS
- Captures volatility clustering effectively
- Limitation: normality assumption may underestimate extreme tails

**3. Historical Simulation**

- Inadequate for dynamic risk management
- Too slow to react to regime shifts
- Risk of both under- and over-estimation
- Not recommended for regulatory capital or risk limits

## Q3: What drives these differences?

**Factor 1: Volatility Dynamics**

Financial returns exhibit **time-varying volatility** (volatility clustering).

- HS ignores this → all observations weighted equally
- GARCH explicitly models $\sigma_t^2$ → recent shocks matter more
- FHS inherits GARCH's volatility dynamics

**Factor 2: Distribution Shape**

Returns have **fatter tails** than normal distribution (leptokurtosis).

- GARCH assumes $z_t \sim N(0,1)$ → may understate tail risk
- FHS uses **empirical** distribution of $z_t$ → captures actual tail thickness
- HS captures tails but with excessive lag

**Factor 3: Non-Stationarity**

Market regimes evolve; parameters shift over time.

- HS: Fixed window, equal weights → slow regime detection
- GARCH: Exponentially-weighted → recent data dominates
- Result: GARCH-based methods are more **adaptive**

**Factor 4: 2021 Specifics**

The testing year featured:

- Pandemic recovery volatility
- Inflation surge
- Monetary policy pivots
- Supply chain shocks

These created **rapid volatility changes** that static HS couldn't track.

# Conclusions

## Key Takeaways

1. **Model selection matters** — especially during turbulent periods

2. **Filtered HS recommended** for practical risk management:
   - Combines best features of parametric and non-parametric approaches
   - Robust to distributional misspecification
   - Responsive to volatility changes

3. **Historical Simulation inadequate** for modern applications:
   - Fails to capture volatility clustering
   - Inappropriate for regulatory compliance
   - Acceptable only for preliminary analysis

4. **GARCH is viable** but consider enhancements:
   - Student-t or skewed-t distributions for better tail fit
   - GJR-GARCH for leverage effects
   - Regular backtesting essential

---

**Analysis Date:** `r Sys.Date()`  
**Software:** R version `r getRversion()`

